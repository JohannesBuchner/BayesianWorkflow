{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bdd18be",
   "metadata": {},
   "source": [
    "## 4. Model development\n",
    "\n",
    "<br>\n",
    "\n",
    "Last time we used simulations as well as prior and posterior predictive checks to check and improve upon our model for Cepheids in a single galaxy. We then found the `alpha` and `beta` posteriors for each galaxy independently. \n",
    "\n",
    "<br>\n",
    "\n",
    "In order to use the period luminosity relation to extend the cosmic distance ladder, and eventually estimate the Hubble constant, the relation needs be *universal*, i.e. the same `alpha` and `beta` should hold for all galaxies, within uncertainties.  \n",
    "\n",
    "<br>\n",
    "\n",
    "From our studies of individual galaxies, we should be able to see the similarities and differences between the results. But, how do we combine these results properly, and keep track of the uncertainties or correlations between parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d21ee",
   "metadata": {},
   "source": [
    "### 4.1. Hierarchical models\n",
    "\n",
    "<br>\n",
    "\n",
    "A natural way to combine the inidividual galaxy results is to build a **hierarchical model** for all galaxies that we have data for. To understand how to do this, we can start with a simpler example of a simple normal model, like the one we used to understand Stan in the [introduction notebook](introduction.ipynb).\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"figures/hierarchical_model.pdf\" alt=\"Going from a single-level normal model to an hierarchical normal model\" width=\"600\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "Imagine that $\\mu$ is now sampled from some parent distribution, with mean $\\theta$ and standard deviation $\\tau$.\n",
    "\n",
    "<br>\n",
    "\n",
    "<u>Single-level normal model:</u>\n",
    "$$\n",
    "p(\\mu, \\sigma|x) \\propto p(x|\\mu, \\sigma) p(\\mu, \\sigma).\n",
    "$$\n",
    "\n",
    "<u>Hierarchical normal model</u>:\n",
    "$$\n",
    "p(\\theta, \\tau | x) \\propto p(x|\\mu, \\sigma) p(\\sigma) p(\\mu | \\theta, \\tau) p(\\theta, \\tau)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Hierarchical models are ideal for describing populations of objects with similar properties, like our Cepheid-hosting galaxies. If we let `alpha` and `beta` for each galaxy come from parent distributions with means `mu_X` and standard deviations `tau_X`, we can then fit for the shape of the universal `alpha` and `beta` distributions!\n",
    "\n",
    "<br>\n",
    " \n",
    "We see that building up hierarchies in this way can quickly lead to a large number of free parameters, which may seem scary. However, Hamiltonian Monte Carlo is designed to cope with these kind of models, and this should be no problem for Stan.\n",
    "\n",
    "<br>\n",
    "\n",
    ">Note: Even though the models we were working with so far often had multiple levels, the relationship between parameters across these levels was purely deterministic (e.g. between m and M). In a true hierarchical model, parameters in the hierarchy have probabilistic relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507f1790",
   "metadata": {},
   "source": [
    "### 4.2. A joint hierarchical fit of all galaxies\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercise 4.1:** Sketch the graphical model for our hierarchical model for Cepheids over all galaxies\n",
    "\n",
    "<br>\n",
    "\n",
    "A simplified Stan model for the full hierarchy can be found in `cepheid_v3.stan`. Note that this is based on `cepheid_v1.stan`, and you should also incorporate the changes that you made during the exercises in the [model checking notebook](model_checking.ipynb) in order to achieve a good fit to the data. I will use `cepheid_v3.stan` here to demonstrate a few important points about hierarchical modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce21844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:39.371496Z",
     "start_time": "2021-08-24T13:35:39.243177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\r\n",
      " * Hierarchical model for Cepheid variables\r\n",
      " * - Multiple galaxies\r\n",
      " * - Shared sigma_m\r\n",
      " * - Weakly informative priors\r\n",
      " **/\r\n",
      "\r\n",
      "data {\r\n",
      "\r\n",
      "  /* usual inputs */\r\n",
      "  int Ng; // number of Galaxies\r\n",
      "  int Nt; // sum(Nc_g)\r\n",
      "  \r\n",
      "  int gal_id[Nt]; // galaxy id for each entry [1 - 9] \r\n",
      "  vector[Nt] m_obs; // obs apparent mag.\r\n",
      "  real sigma_m; // mag uncertainty\r\n",
      "  vector[Nt] log10P; // log10(periods/days)\r\n",
      "  vector[Ng] z; // redshift of single galaxy\r\n",
      "\r\n",
      "}\r\n",
      "\r\n",
      "transformed data {\r\n",
      "\r\n",
      "  vector[Ng] dL;\r\n",
      "  \r\n",
      "  /* luminosity distance */\r\n",
      "  dL = (3.0e5 * z) / 70.0; // Mpc\r\n",
      "  \r\n",
      "}\r\n",
      "\r\n",
      "parameters {\r\n",
      "\r\n",
      "  /* parameters of the parent distributions */\r\n",
      "  real mu_alpha;\r\n",
      "  real<lower=0> tau_alpha;\r\n",
      "\r\n",
      "  real mu_beta;\r\n",
      "  real<lower=0> tau_beta;\r\n",
      "\r\n",
      "  /* individual galaxy parameters */\r\n",
      "  vector[Ng] alpha;\r\n",
      "  vector[Ng] beta;\r\n",
      "  \r\n",
      "}\r\n",
      "\r\n",
      "transformed parameters {\r\n",
      "\r\n",
      "  vector[Nt] M_true;\r\n",
      "  vector[Nt] m_true;\r\n",
      "\r\n",
      "  /* P-L relation */\r\n",
      "  M_true = alpha[gal_id] + beta[gal_id] .* log10P;\r\n",
      "\r\n",
      "  /* convert to m */\r\n",
      "  m_true = M_true + 5 * log10(dL[gal_id]) + 25;\r\n",
      "    \r\n",
      "}\r\n",
      "\r\n",
      "model {  \r\n",
      "\r\n",
      "  /* priors */\r\n",
      "  mu_alpha ~ normal(0, 10);\r\n",
      "  mu_beta ~ normal(-5, 5);\r\n",
      "  tau_alpha ~ cauchy(0, 2.5);\r\n",
      "  tau_beta ~ cauchy(0, 2.5);\r\n",
      "\r\n",
      "  /* connection to latent params */\r\n",
      "  alpha ~ normal(mu_alpha, tau_alpha);\r\n",
      "  beta ~ normal(mu_beta, tau_beta);\r\n",
      "  \r\n",
      "  /* connection to data */\r\n",
      "  m_obs ~ normal(m_true, sigma_m);\r\n",
      "\r\n",
      "}\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat stan/cepheid_v3.stan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eae5d1",
   "metadata": {},
   "source": [
    "A few things worth noting:\n",
    "* The easiest way to deal with a ragged array structure (different number of Cepheids in each galaxy) in Stan is to flatten the array, and pass a vector with the galaxy IDs.\n",
    "* We introduce `mu` and `tau` parameters for `alpha` and `beta`, with sensible priors.\n",
    "* We assume the parent `alpha` and `beta` distributions are independent, as a starting point.\n",
    "* In a hierarchical model, it is not so clear how to separate the priors and likelihood, the model is more a series of conditional steps.\n",
    "\n",
    "<br>\n",
    "\n",
    "Now, let's see how the model performs when fitting out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8413c59e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:40.709588Z",
     "start_time": "2021-08-24T13:35:39.842361Z"
    }
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import arviz as av\n",
    "from cmdstanpy import CmdStanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "876e21f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:40.750603Z",
     "start_time": "2021-08-24T13:35:40.711255Z"
    }
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "galaxy_info = np.loadtxt(\"data/galaxies.dat\")\n",
    "cepheid_info = np.loadtxt(\"data/cepheids.dat\")\n",
    "\n",
    "# compile data from all galaxies\n",
    "galaxies = [int(ngc_no) for ngc_no in galaxy_info[:, 0]]\n",
    "data = {int(x[0]):{'z':x[1]} for x in galaxy_info}\n",
    "with h5py.File(\"data/my_cepheid_data.h5\", \"r\") as f:\n",
    "    for g in galaxies:\n",
    "        group = f[str(g)]\n",
    "        for k in group:\n",
    "            data[g][k] = group[k][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b95b734d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:40.757087Z",
     "start_time": "2021-08-24T13:35:40.752062Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select out two galaxies\n",
    "sel = 2\n",
    "\n",
    "# prepare Stan input\n",
    "input_data = {}\n",
    "input_data[\"Ng\"] = len(galaxies[0:sel])\n",
    "input_data[\"Nt\"] = sum([data[g][\"Nc\"] for g in galaxies[0:sel]])\n",
    "input_data[\"m_obs\"] = np.concatenate([data[g][\"m_obs\"] for g in galaxies[0:sel]])\n",
    "input_data[\"log10P\"] = np.concatenate([data[g][\"log10P_true\"] for g in galaxies[0:sel]])\n",
    "input_data[\"gal_id\"] = np.concatenate([np.tile(i, data[g][\"Nc\"]) \n",
    "                                       for i, g in enumerate(galaxies[0:sel])]) + 1\n",
    "input_data[\"z\"] = galaxy_info.T[1][0:sel]\n",
    "input_data[\"sigma_m\"] = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84f939d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:51.268696Z",
     "start_time": "2021-08-24T13:35:40.792951Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:compiling stan program, exe file: /Users/fran/projects/bayesian_workflow_prep/stan/cepheid_v3\n",
      "INFO:cmdstanpy:compiler options: stanc_options=None, cpp_options=None\n",
      "INFO:cmdstanpy:compiled model file: /Users/fran/projects/bayesian_workflow_prep/stan/cepheid_v3\n"
     ]
    }
   ],
   "source": [
    "stan_model = CmdStanModel(stan_file=\"stan/cepheid_v3.stan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f13ac130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:52.666712Z",
     "start_time": "2021-08-24T13:35:51.271670Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:start chain 1\n",
      "INFO:cmdstanpy:start chain 2\n",
      "INFO:cmdstanpy:start chain 3\n",
      "INFO:cmdstanpy:start chain 4\n",
      "INFO:cmdstanpy:finish chain 4\n",
      "INFO:cmdstanpy:finish chain 3\n",
      "INFO:cmdstanpy:finish chain 2\n",
      "INFO:cmdstanpy:finish chain 1\n"
     ]
    }
   ],
   "source": [
    "fit = stan_model.sample(data=input_data, chains=4, iter_sampling=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788806e2",
   "metadata": {},
   "source": [
    "We should always check the sampler diagnostics first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4549d03e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:54.611730Z",
     "start_time": "2021-08-24T13:35:52.668169Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:cmdstanpy:Processing csv files: /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmpkrh3070j/cepheid_v3-202108241535-1-siwu8qu5.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmpkrh3070j/cepheid_v3-202108241535-2-80wmkq_t.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmpkrh3070j/cepheid_v3-202108241535-3-63ca8qiy.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmpkrh3070j/cepheid_v3-202108241535-4-rcx1e0t7.csv\n",
      "\n",
      "Checking sampler transitions treedepth.\n",
      "Treedepth satisfactory for all transitions.\n",
      "\n",
      "Checking sampler transitions for divergences.\n",
      "24 of 4000 (0.6%) transitions ended with a divergence.\n",
      "These divergent transitions indicate that HMC is not fully able to explore the posterior distribution.\n",
      "Try increasing adapt delta closer to 1.\n",
      "If this doesn't remove all divergences, try to reparameterize the model.\n",
      "\n",
      "Checking E-BFMI - sampler transitions HMC potential energy.\n",
      "E-BFMI satisfactory.\n",
      "\n",
      "Effective sample size satisfactory.\n",
      "\n",
      "Split R-hat values satisfactory all parameters.\n",
      "\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "fit.diagnose();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22510e85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-24T13:35:57.023355Z",
     "start_time": "2021-08-24T13:35:54.614584Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>MCSE</th>\n",
       "      <th>StdDev</th>\n",
       "      <th>5%</th>\n",
       "      <th>50%</th>\n",
       "      <th>95%</th>\n",
       "      <th>N_Eff</th>\n",
       "      <th>N_Eff/s</th>\n",
       "      <th>R_hat</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lp__</th>\n",
       "      <td>-720.00</td>\n",
       "      <td>0.09100</td>\n",
       "      <td>2.600</td>\n",
       "      <td>-720.00</td>\n",
       "      <td>-720.00</td>\n",
       "      <td>-710.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_alpha</th>\n",
       "      <td>-0.85</td>\n",
       "      <td>0.09200</td>\n",
       "      <td>2.200</td>\n",
       "      <td>-3.20</td>\n",
       "      <td>-0.81</td>\n",
       "      <td>1.3</td>\n",
       "      <td>550.0</td>\n",
       "      <td>290.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau_alpha</th>\n",
       "      <td>1.80</td>\n",
       "      <td>0.12000</td>\n",
       "      <td>2.600</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.10</td>\n",
       "      <td>5.5</td>\n",
       "      <td>490.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mu_beta</th>\n",
       "      <td>-4.30</td>\n",
       "      <td>0.04300</td>\n",
       "      <td>1.800</td>\n",
       "      <td>-7.10</td>\n",
       "      <td>-4.20</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>1600.0</td>\n",
       "      <td>850.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tau_beta</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.07200</td>\n",
       "      <td>2.500</td>\n",
       "      <td>0.71</td>\n",
       "      <td>1.80</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1200.0</td>\n",
       "      <td>630.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_true[295]</th>\n",
       "      <td>28.00</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>0.081</td>\n",
       "      <td>28.00</td>\n",
       "      <td>28.00</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>958.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_true[296]</th>\n",
       "      <td>27.00</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>0.068</td>\n",
       "      <td>27.00</td>\n",
       "      <td>27.00</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>981.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_true[297]</th>\n",
       "      <td>24.00</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2984.0</td>\n",
       "      <td>1544.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_true[298]</th>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>0.050</td>\n",
       "      <td>20.00</td>\n",
       "      <td>20.00</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3084.0</td>\n",
       "      <td>1595.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m_true[299]</th>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00054</td>\n",
       "      <td>0.036</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.00</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4425.0</td>\n",
       "      <td>2289.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>607 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Mean     MCSE  StdDev      5%     50%    95%   N_Eff  N_Eff/s  \\\n",
       "name                                                                           \n",
       "lp__        -720.00  0.09100   2.600 -720.00 -720.00 -710.0   800.0    410.0   \n",
       "mu_alpha      -0.85  0.09200   2.200   -3.20   -0.81    1.3   550.0    290.0   \n",
       "tau_alpha      1.80  0.12000   2.600    0.30    1.10    5.5   490.0    250.0   \n",
       "mu_beta       -4.30  0.04300   1.800   -7.10   -4.20   -1.5  1600.0    850.0   \n",
       "tau_beta       2.50  0.07200   2.500    0.71    1.80    6.5  1200.0    630.0   \n",
       "...             ...      ...     ...     ...     ...    ...     ...      ...   \n",
       "m_true[295]   28.00  0.00190   0.081   28.00   28.00   29.0  1851.0    958.0   \n",
       "m_true[296]   27.00  0.00160   0.068   27.00   27.00   28.0  1897.0    981.0   \n",
       "m_true[297]   24.00  0.00072   0.039   24.00   24.00   24.0  2984.0   1544.0   \n",
       "m_true[298]   20.00  0.00091   0.050   20.00   20.00   21.0  3084.0   1595.0   \n",
       "m_true[299]   23.00  0.00054   0.036   23.00   23.00   23.0  4425.0   2289.0   \n",
       "\n",
       "             R_hat  \n",
       "name                \n",
       "lp__           1.0  \n",
       "mu_alpha       1.0  \n",
       "tau_alpha      1.0  \n",
       "mu_beta        1.0  \n",
       "tau_beta       1.0  \n",
       "...            ...  \n",
       "m_true[295]    1.0  \n",
       "m_true[296]    1.0  \n",
       "m_true[297]    1.0  \n",
       "m_true[298]    1.0  \n",
       "m_true[299]    1.0  \n",
       "\n",
       "[607 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd1a1c",
   "metadata": {},
   "source": [
    "Hmmm, it seems something is off. Let's check the trace and pair plots..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddba7ca",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:42.692Z"
    }
   },
   "outputs": [],
   "source": [
    "av.plot_trace(fit, var_names=[\"mu_alpha\", \"mu_beta\", \"alpha\", \"beta\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46c704",
   "metadata": {},
   "source": [
    "To make a sensibly-sized pair plot and view divergent transitions, we have to pass some more information to `arviz`. Let's focus on the relationship between the `alpha` and `beta` hyperparameters and `M_true` latent parameters for now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ba2f76",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:43.304Z"
    }
   },
   "outputs": [],
   "source": [
    "fit_a = av.from_cmdstanpy(fit, coords={\"Ng\" : np.arange(input_data[\"Ng\"]), \n",
    "                                       \"Nt\" : np.arange(input_data[\"Nt\"])}, \n",
    "                             dims={\"alpha\": [\"Ng\"], \"beta\": [\"Ng\"], \"M_true\":[\"Nt\"]})\n",
    "av.plot_pair(fit_a, var_names=[\"M_true\", \"alpha\", \"beta\"], \n",
    "                divergences=True, coords={\"Ng\": np.array([0, 1]), \n",
    "                                          \"Nt\": np.array([0])});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1c889f",
   "metadata": {},
   "source": [
    "The goemetry of a hierarchical model with its many latent parameters is much more complex than that of a single model, and divergent transitions can occur. Fortunately, these can be used to help us understand and fix our model.\n",
    "\n",
    "Here, we can see that the misspecification of the magnitude uncertainty causes problems, although these issues become less servere if we add data from more galaxies, as I'll now demonstrate. It is important to remember that **the geometry the sampler sees depends on both the model and the data**. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercsie 4.2.1:** Update the above Stan code with your improvements from the single galaxy case that you carried out in the [model checking notebook](model_checking.ipynb) and display the code. Verify that you can run the above demo with no divergent transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdaf3c6",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:43.799Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122c3dc0",
   "metadata": {},
   "source": [
    "**Exercise 4.2.2:** Perform a joint fit of all the galaxies using your improved model, and visualise the posterior predictive distribution of your fit. Can you fit the data well? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2075a8b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:44.327Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c119b92d",
   "metadata": {},
   "source": [
    "**Exercise 4.2.3:** Compare the galaxy-level `alpha` and `beta` distributions to those found from individual fits that you completed as part of the [model checking notebook](model_checking.ipynb). What differences can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303c6a7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:44.858Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37d1245",
   "metadata": {},
   "source": [
    "**Exercise 4.2.4:** Make a plot to show how the constraints on the parent distributions for `alpha` and `beta` change depending on the number of galaxies used in the sample. What happens when we only use one or two galaxies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009bc8db",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:45.848Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a963a8",
   "metadata": {},
   "source": [
    "### 4.3. Estimation of the Hubble constant\n",
    "\n",
    "<br>\n",
    "\n",
    "So far we have assumed a value for `H0` in our fits in order to convert between apparent and absolute magnitude. This is necessary to fit for `alpha` and `beta`. Let's think about how we can extend the model to make an estimate of `H0`, rather than assuming it.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Exercise 4.3.1:** What happens if we simply leave `H0` as a free parameter in our existing hierarchical model? Why is this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fadb73b",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:46.695Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e00c8fd",
   "metadata": {},
   "source": [
    "**Exercise 4.3.2:** What happens if we change the value of `H0` assumed in our analysis? Which results change, and which stay the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6e5443",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:47.183Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb98933",
   "metadata": {},
   "source": [
    "The Cepheid variable experts that we talked to earlier are impressed with our careful analysis of the period-luminosity relation. They share a new dataset of Cepheid variable stars that are in our own Galaxy, the Milky Way. The distances to these stars are well-measured using geometric methods.\n",
    "\n",
    "<br>\n",
    "\n",
    "This dataset is given in `milkyway_cepheids.dat`. It similar to that we have seen before, except that each Cepheid has a distance measurement in kpc. We are also told that the uncertainty in the distance measurements is 150 pc, and that on the apparent magnitudes is 0.1, for all Cepheids. Any effect due to metallicity can be assumed to be negligible here.\n",
    "\n",
    "<br>\n",
    "\n",
    "Let's take a look a the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a57db5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:47.904Z"
    }
   },
   "outputs": [],
   "source": [
    "mw_cepheids = np.loadtxt(\"data/milkyway_cepheids.dat\")\n",
    "d = mw_cepheids.T[0] # distance in kpc\n",
    "m_obs = mw_cepheids.T[1] # apparent mag\n",
    "P = mw_cepheids.T[2] # period in days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805bfeaa",
   "metadata": {},
   "source": [
    "**Exercise 4.3.1:** Use this new data set to make an independent estimate of `alpha` and `beta`, without assuming `H0`, what do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a11377c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-08-24T13:35:48.784Z"
    }
   },
   "outputs": [],
   "source": [
    "# to be completed..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804e5b2",
   "metadata": {},
   "source": [
    "**Exercise 4.3.2:** Update the hierarchical model that you have to include key information from the calibration data into your joint fit. If done correctly, this should allow you to break the degeneracy in the model and directly estimate the Hubble constant. Try to include all relevant uncertainties.\n",
    "\n",
    ">**Hint**: Assume that `alpha` from the Milky Way dataset is a fair representaion of `mu_alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bb06e",
   "metadata": {},
   "source": [
    "**Exercise 4.3.4:** What factors affect the uncertainties in your estimate? How could you get a more constrained result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc42baf5",
   "metadata": {},
   "source": [
    "### Further reading\n",
    "Hierarchical models: Chapter 5 in [Bayesian data analysis](http://www.stat.columbia.edu/~gelman/book/) by Gelman et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c03da0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayesian_workflow",
   "language": "python",
   "name": "bayesian_workflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
