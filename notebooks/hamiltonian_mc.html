

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Hamiltonian Monte Carlo &mdash; Bayesian workflow  documentation</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="6. Experiment design" href="experiment_design.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> Bayesian workflow
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_building.html">2. Model building</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_checking.html">3. Model checking</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_development.html">4. Model development</a></li>
<li class="toctree-l1"><a class="reference internal" href="model_comparison.html">5. Model comparison</a></li>
<li class="toctree-l1"><a class="reference internal" href="experiment_design.html">6. Experiment design</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Hamiltonian Monte Carlo</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Computing-expectations-with-MCMC">Computing expectations with MCMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="#The-basics-of-HMC">The basics of HMC</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Tuning">Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Implementation">Implementation</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Diagnostics">Diagnostics</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Demonstration">Demonstration</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Bayesian workflow</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Hamiltonian Monte Carlo</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/notebooks/hamiltonian_mc.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="Hamiltonian-Monte-Carlo">
<h1>Hamiltonian Monte Carlo<a class="headerlink" href="#Hamiltonian-Monte-Carlo" title="Permalink to this headline">¶</a></h1>
<p>The MCMC samplers that we have discussed so far can be thought of as a guided random walk towards regions of high posterior density. This random walk behaviour is inherently <strong>inefficient</strong>. Model reparametrisation and other tricks can help to improve the situation, but the inefficiency remains, <strong>especially in high-dimensional problems</strong>.</p>
<p><strong>Hamiltonian Monte Carlo (HMC)</strong> is a dramatically more efficient way to draw samples from the target posterior distribution, thanks to its departure from the random walk approach. Instead, HMC uses concepts from Hamiltonian mechanics to direct the Markov transitions and avoid diffusive behaviour.</p>
<p>An excellent introduction to HMC that is suitable for researchers in the physical sciences can be found in <a class="reference external" href="https://arxiv.org/pdf/1701.02434.pdf">this paper</a> by Michael Betancourt. Here, I will try to summarise the main concepts, borrowing notation, figures and explanations from this work.</p>
<section id="Computing-expectations-with-MCMC">
<h2>Computing expectations with MCMC<a class="headerlink" href="#Computing-expectations-with-MCMC" title="Permalink to this headline">¶</a></h2>
<p>The goal in the implementation of any MCMC algorithm is the computation of expectation values. Consider a target sample space <span class="math notranslate nohighlight">\(Q\)</span>, of which any point <span class="math notranslate nohighlight">\(q \in Q\)</span> can be parametrised by real numbers, in <span class="math notranslate nohighlight">\(D\)</span> dimensions. In this parameter space, let our target distribution be a smooth density function, <span class="math notranslate nohighlight">\(\pi(q)\)</span>.</p>
<p>Computing expectation values over this distribution is done by integrating over the parameter space</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}_\pi[f] = \int_Q \mathrm{d}q~\pi(q)f(q).\]</div>
<p>In practice, we approximate these integrals through the Markov chains returned by our samplers.</p>
<p>Given that we want to compute expectations, an obvious way for our sampler to be inefficient is to <strong>waste time evaluating :math:`pi(q)` in regions of parameter space that have negligible contribution to the expectation.</strong></p>
<p>So we should just focus on regions with large density, <span class="math notranslate nohighlight">\(\pi(q)\)</span>, right? <em>Not exactly</em>. The expectation is an <em>integral</em> of this density of the <em>volume</em>, <span class="math notranslate nohighlight">\(\mathrm{d}q\)</span>. While the density will be largest at the mode, there is relatively little volume here, especially in high dimensions. Recall the curse of dimensionality as illustrated in the figure below. The relative weight of a partition containing a point of interest decreases from <span class="math notranslate nohighlight">\(1/3\)</span> to <span class="math notranslate nohighlight">\(1/9\)</span> and <span class="math notranslate nohighlight">\(1/27\)</span> as we
scale from 1 to 3 dimensions.</p>
<p><img alt="The curse of dimensionality" class="no-scaled-link" src="../_images/hmc1.png" style="width: 800px;" /></p>
<p>Actually, volume will be largest in the tails of the distribution, away from the mode. If we have <span class="math notranslate nohighlight">\(\pi(q)\)</span> largest at the mode, and <span class="math notranslate nohighlight">\(\mathrm{d}q\)</span> largest in the tails, the region of interest is somewhere between the two. Let’s refer to this region as the <strong>typical set</strong>.</p>
<p><img alt="The typical set" class="no-scaled-link" src="../_images/hmc2.png" style="width: 500px;" /></p>
<p>So we have understood that we want to focus all our computational resources on exploring this typical set. As we discussed above, standard MCMC algorithms, such as the Metropolis-Hastings algorithm, will eventually explore the typical set, as long as the Markov transition <em>preserves</em> the target distribution.</p>
<p>Given sufficient time, samples from the Markov chain can be used to approximate our desired expectations:</p>
<div class="math notranslate nohighlight">
\[\hat{f}_N = \frac{1}{N} \sum_{n=0}^N f(q_n),\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\lim_{N\to\infty} \hat{f}_N = \mathbb{E}_\pi[f].\]</div>
</section>
<section id="The-basics-of-HMC">
<h2>The basics of HMC<a class="headerlink" href="#The-basics-of-HMC" title="Permalink to this headline">¶</a></h2>
<p>So how can we explore the typical set more efficiently than with just randomly walking? We can exploit information about the <em>geometry</em> of the typical set, and use this to move through it. Imagine that we knew the <em>vector field</em> of the typical set, we could just follow it, like a ball rolling along a slope.</p>
<p>Great! But how can we find this vector field using only information about our target density? The <em>gradient</em> of the density will give us a vector field that points towards the mode, but we want to move around the mode in the typical set. We can imagine this as a physical system, like a satellite (our trajectory) in orbit around the Earth (the mode):</p>
<p><img alt="Exploring the typical set" class="no-scaled-link" src="../_images/hmc3.png" style="width: 900px;" /></p>
<p>We too can avoid “crashing into the mode” by giving our “satellite” enough momentum. But we don’t want to add too much or too little momentum either. Let’s start by expanding our parameter space by introducing auxiliary momentum parameters, <span class="math notranslate nohighlight">\(p\)</span></p>
<div class="math notranslate nohighlight">
\[q \rightarrow (q, p).\]</div>
<p>In doing this, our target distribution is now a joint probability distrbution over <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(q\)</span>. We can express this as</p>
<div class="math notranslate nohighlight">
\[\pi(q, p) = \pi(p|q)\pi(q).\]</div>
<p>This choice ensure that if we marginalise out the momentum, we immediately recover what we started with. Any trajectory in this new joint space can therefore be projected down into our original parameter space.</p>
<p>Continuing the analogy with classical mechanics, we can now use Hamiltonian dynamics to construct trajectories in the joint space. The Hamiltonian energy of the system</p>
<div class="math notranslate nohighlight">
\[H(q, p) \equiv -\log\pi(q, p),\]</div>
<p>is composed of two terms</p>
<div class="math notranslate nohighlight">
\[H(q, p) = - \log\pi(p|q) - \log\pi(q) \equiv K(p, q) + V(q),\]</div>
<p>with K(p, q) and V(q) the kinetic and potential energies, respectively. The potential energy is set by the target distribution, whereas the kinetic energy is to be determined.</p>
<p>The Hamiltonian contains the geometry of the typical set, and so we can use it to find a vector field aligned with the typical set via <strong>Hamilton’s equations</strong>,</p>
<div class="math notranslate nohighlight">
\[\frac{\mathrm{d}q}{\mathrm{d}t} = \frac{\partial H}{\partial p} = \frac{\partial K}{\partial p}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\mathrm{d}p}{\mathrm{d}t} = - \frac{\partial H}{\partial q} = - \frac{\partial K}{\partial q} - \frac{\partial V}{\partial q}.\]</div>
<p>Here we recognise <span class="math notranslate nohighlight">\(\partial V / \partial q\)</span> as simply the gradient of our target distribution, satisfying our above physical intuition.</p>
<blockquote>
<div><p>Evolving Hamilton’s equations for some time generates trajectories in the joint space that efficiently move around the typical set. Projecting these trajectories back into our parameter space via marginalisation gives what we set out to find!</p>
</div></blockquote>
<p>The Hamiltonian Markov transition * Lift a point into the joint space from the parameter space by sampling from the conditional distribution</p>
<div class="math notranslate nohighlight">
\[p \sim \pi(p | q).\]</div>
<p>* Integrate Hamilton’s equations for some time to move around the typical set</p>
<div class="math notranslate nohighlight">
\[(q, p) \rightarrow \phi_t(q, p).\]</div>
<p>* Project back down into parameter space via marginalisation</p>
<div class="math notranslate nohighlight">
\[(q, p) \rightarrow q.\]</div>
<p>A nice interactive visualisation of the differences in a variety of sampling algorithms can be found on <a class="reference external" href="https://chi-feng.github.io/mcmc-demo/app.html">this website</a>, by Chi Feng.</p>
</section>
<section id="Tuning">
<h2>Tuning<a class="headerlink" href="#Tuning" title="Permalink to this headline">¶</a></h2>
<p>In our above explanation, we left two quantities to be determined: * The <strong>kinetic energy</strong> <span class="math notranslate nohighlight">\(\sim \pi(p|q)\)</span>. * The <strong>integration time</strong>, <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>These degrees of freedom need to be tuned to suit our particular problem.</p>
<p>Optimising the kinetic energy</p>
<p>Each time we sample a new momentum, we enter a different <em>energy level</em> of the system. For efficient exploration, we want to explore different energy levels in a uniform way, and for our momentum sampling to imply energy sampling that is close to that of the marginal energy distribution of the system. I.e.</p>
<div class="math notranslate nohighlight">
\[\pi(p|q) \Rightarrow \pi(E|q),\]</div>
<p>and we want</p>
<div class="math notranslate nohighlight">
\[\pi(E|q) \sim \pi(E).\]</div>
<p>As there are an infinite number of possible kinietic energies, it makes sense to restrict our search. A standard choice are <em>Euclidean-Gaussian</em> kinetic energies, such that</p>
<div class="math notranslate nohighlight">
\[\pi(p|q) = \mathcal{N}(p | 0, M),\]</div>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is typically referred to as the <em>mass matrix</em>. Because of the dual behaviour between the momentum and parameters, it turns out we can define an optimal choice for <span class="math notranslate nohighlight">\(M\)</span> by setting its inverse equal to the <em>target covariances</em>. In practice, this is estimated during the warmup phase of an HMC algorithm implementation.</p>
<p>Optimising the integration time</p>
<p>If we integrate for short times, we won’t explore so far away, but if we integrate for too long, we will end up revisiting regions that have already been explored. The optimal choice will depend on the kinetic energy and where in the joint space the transition occurs.</p>
<p>A practical solution to this is the <a class="reference external" href="https://jmlr.org/papers/volume15/hoffman14a/hoffman14a.pdf">No-U-Turn</a> trajectory termination criterion that allows the intergation time to be chosen dynamically. Samplers implementing this criterion are often reffered to as No-U-Turn samplers, or NUTS.</p>
</section>
<section id="Implementation">
<h2>Implementation<a class="headerlink" href="#Implementation" title="Permalink to this headline">¶</a></h2>
<p>In practice, we have to solve Hamilton’s equations <em>numerically</em>. This leads to inaccuracies that add with integration time and scale with the number of dimensions. The geometry of the problem motivates the choice of <strong>Symplectic integrators</strong> for which the errors at least oscillate around the true values. Another aspect of their performance is that in regions of extreme curvature, the numerical trajectories diverge to infinity.</p>
<p><img alt="Divergent transitions" class="no-scaled-link" src="../_images/hmc4.pdf" style="width: 900px;" /></p>
<p>While this may seem like a bad thing, the existence and location of <strong>divergent transitions</strong> is actually a very helpful diagnostic when implementing HMC in practice.</p>
<p>We can also correct for the error induced by integration by defining a <strong>reversible transition</strong> (positive and negative momentum) and introducing an <strong>accept/reject Metropolis step</strong>, as the acceptance probability can be calculated exactly from the Hamiltonian.</p>
<p>As we integrate numerically, the integration time becomes defined by a <strong>step size</strong> and <strong>number of steps</strong>. The step size can be optimised during the warmup phase, by defining a target metropolis acceptance rate (typically around 0.8). The number of steps, or trajectory length, is optimised dynamically during sampling via the No-U-Turn criterion mentioned above.</p>
<p>Stan</p>
<p><a class="reference external" href="https://mc-stan.org">Stan</a> is a software platform with a robust implementation of an adaptive HMC algorithm, including NUTS. Given a model specification and data, Stan will automatically optimise the <em>mass matrix</em> and <em>step size</em>, letting you focus on your model and inferences.</p>
<p>We will work more with Stan in the second block of the course as a tool for implementing a Bayesian workflow.</p>
</section>
<section id="Diagnostics">
<h2>Diagnostics<a class="headerlink" href="#Diagnostics" title="Permalink to this headline">¶</a></h2>
<p>Like any MCMC algorithm, the <strong>effective sample size</strong> and <strong>Gelman-Rubin statistic</strong> can be used to judge the within-chain correlation and convergence of chains. The implementation of HMC also permits two new diagnostic tools:</p>
<ul class="simple">
<li><p><strong>Divergent transitions:</strong> As mentioned above, in regions of high curvature the results of numerical integration will diverge. We can use divergent transitions to notify us of the presence and also location of regions of high curvature.</p></li>
<li><p><strong>E-BFMI:</strong> The energy Bayesian fraction of missing information quantifies the mismatch between <span class="math notranslate nohighlight">\(\pi(E|q)\)</span> and <span class="math notranslate nohighlight">\(\pi(E)\)</span>, allowing us to diagnose poorly-chosen kinetic energies. If the E-BFMI is below around 0.3, it seems that the warmup is unable to find a decent mass matrix for the presented problem.</p></li>
</ul>
<p>If these kind of divergences are encountered, it is likely that some form of model reparametrisation can help out.</p>
</section>
<section id="Demonstration">
<h2>Demonstration<a class="headerlink" href="#Demonstration" title="Permalink to this headline">¶</a></h2>
<p>We can demonstrate the power of these diagnostics with using Stan and the classic <em>Eight schools problem</em>. See Section 5.5. of <a class="reference external" href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a> by Gelman et al. for more information. The initial model is given in <code class="docutils literal notranslate"><span class="pre">stan/schools.stan</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>import numpy as np
import arviz as av
from cmdstanpy import CmdStanModel
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>!cat stan/schools.stan
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
data {
  int&lt;lower=0&gt; J;
  real y[J];
  real&lt;lower=0&gt; sigma[J];
}

parameters {
  real mu;
  real&lt;lower=0&gt; tau;
  real theta[J];
}

model {
  mu ~ normal(0, 5);
  tau ~ cauchy(0, 5);
  theta ~ normal(mu, tau);
  y ~ normal(theta, sigma);
}
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>model = CmdStanModel(stan_file=&quot;stan/schools.stan&quot;)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
INFO:cmdstanpy:found newer exe file, not recompiling
INFO:cmdstanpy:compiled model file: /Users/fran/projects/bayesian_workflow_prep/stan/schools
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>data = {}
data[&quot;J&quot;]= 8
data[&quot;y&quot;] = [28,  8, -3,  7, -1,  1, 18, 12]
data[&quot;sigma&quot;] = [15, 10, 16, 11,  9, 11, 10, 18]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>fit = model.sample(data=data, iter_sampling=1000, chains=4)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
INFO:cmdstanpy:start chain 1
INFO:cmdstanpy:start chain 2
INFO:cmdstanpy:start chain 3
INFO:cmdstanpy:start chain 4
INFO:cmdstanpy:finish chain 1
INFO:cmdstanpy:finish chain 3
INFO:cmdstanpy:finish chain 2
INFO:cmdstanpy:finish chain 4
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>fit.diagnose();
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
INFO:cmdstanpy:Processing csv files: /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmppsjxm3x8/schools-202108251331-1-l6tu2748.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmppsjxm3x8/schools-202108251331-2-7f36w6fr.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmppsjxm3x8/schools-202108251331-3-9u0p2ltb.csv, /var/folders/8d/cyg0_lx54mggm8v350vlx91r0000gn/T/tmppsjxm3x8/schools-202108251331-4-qu5i5n6z.csv

Checking sampler transitions treedepth.
Treedepth satisfactory for all transitions.

Checking sampler transitions for divergences.
37 of 4000 (0.93%) transitions ended with a divergence.
These divergent transitions indicate that HMC is not fully able to explore the posterior distribution.
Try increasing adapt delta closer to 1.
If this doesn&#39;t remove all divergences, try to reparameterize the model.

Checking E-BFMI - sampler transitions HMC potential energy.
The E-BFMI, 0.25, is below the nominal threshold of 0.3 which suggests that HMC may have trouble exploring the target distribution.
If possible, try to reparameterize the model.

Effective sample size satisfactory.

Split R-hat values satisfactory all parameters.

Processing complete.
</pre></div></div>
</div>
<p>Note the presence of divergent transitions and low E-BFMI. Let’s visualise the location of the divergent transitions.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>fit_a = av.from_cmdstanpy(fit, coords={&quot;J&quot; : np.arange(data[&quot;J&quot;])},
                          dims={&quot;theta&quot;: [&quot;J&quot;]})
av.plot_pair(fit_a, var_names=[&quot;theta&quot;, &quot;tau&quot;, &quot;mu&quot;],
             divergences=True, coords={&quot;J&quot;: np.array([0, 1])});
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../_images/notebooks_hamiltonian_mc_14_0.png" src="../_images/notebooks_hamiltonian_mc_14_0.png" />
</div>
</div>
<p>We see that the divergent transitions are clustered around small tau values before an abrupt stop. This indicates a funnel geometry that the sampler is not able to penetrate.</p>
<p>This is a well-known problem for models of this type. We can try to reparametrise our model using a <a class="reference external" href="https://mc-stan.org/docs/2_18/stan-users-guide/reparameterization-section.html">non-centred parametrisation</a>.</p>
<p><strong>Live demo:</strong> Implement a non-centred version of the eight schools model and verify that that all diagnostic checks can now pass.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span>
</pre></div>
</div>
</div>
</section>
</section>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="experiment_design.html" class="btn btn-neutral float-left" title="6. Experiment design" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Francesca Capel.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>